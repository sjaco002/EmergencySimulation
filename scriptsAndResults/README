Two files are altered in the one simulator:
1) default_settings.txt
2) DTNHost.java
ALL OTHER FILES ARE DEFAULT ONE SIMULATOR FILES

Output to logs happens in DTNHost.update:
200 shelters are created at time 0.1
10 emergencies happen every 10 seconds produced by 10 different rapidly moving users:
	3 fires
	5 floods
	1 crash
	1 storm 
The one simulator doesn't give them a radius, so this should be done in convertToADM.py
One idea for radius values would be (city is around 4,200 by 3,200 and starts at 0,0)
flood:1000
fire:500
storm:2000
crash:100

Channel returns shelters that are within a radius of 300 (should be 3 or 4 on average)


Each user has a location every 100 second
Six groups of users: 2 pedestrians groups, 1 driving group, and 3 metro groups
	Each group has 835 nodes, with each node representing 100 users, for a total of 501,000 users on 5,010 unique paths

usernames are (p|c|w|t) + (0-5009) + u + (00-99)
p has 0-834
c has 835-1669
w has 1670-2504
t has 2505-5009

Logs are output to file specified in logging.properties


generatedate.sh:
1) Compiles and runs the one simulator in ../oneSimulator to produce output file with all three datasets logged
2) Converts the output file into a seperate ADM file for each dataset (reports, locations, shelters)

convertToADM.py:
1) turns the log into individual ADM output

Once those two are done, do the following steps:

For Channel Version:
1) initializeDatasets.sh
2) startChannel.sh
ON BROKER:
3) python startResultsQuery.py
4) python averageResultsTime.py to see results

For Polling Version:
1) initializeDatasets.sh
ON BROKER:
2) rm -rf results.txt && python pollSingleThread.py >> results.txt
3) python averagePollSingleThread.py to see results


To send data to the feeds:
1) sendLocations.py (takes a parameter for the number of users)
2) sendReports.py (takes a parameter for the number of reports per second)


makeSubscriptionsFile.py
1) Takes two parameters: the number of users and the number of brokers [1,2,3]
2) creates a subscriptions file for that setting as subscriptionsXbrokerY.adm


initializeDatasets.sh:
1) Creates datasets 
2) Fills Shelters dataset
3) Starts channels for userlocations and EmergencyReports

createSubscriptions.sh
1) Generates a subscriptions file to bulkload subscriptions
Use the following command: ./createSubscriptions.sh && scp subscriptions.adm promethium:two/.

createSubscriptions.py
1) Generates a subscriptions file to bulkload subscriptions
Use the following command: python createSubscriptions.py && scp subscriptions.adm promethium:two/.

startChannel.sh
1) Starts the channel going
2) Bulkloads Subscriptions

sendIndividualSubscriptions.sh
1) Can be used to send subscriptions 1-by-1 if you don't want all 501000

statisticsQueries.sqlpp
1) Queries to see how many things happened in the last 10 seconds

sendLocations.py
1) Sends User Locations over time

sendReports.py
1) Sends Reports over time


RUN THE REST OF THESE ON THE BROKER:
startClients.py
1) Sends individual User Queries as separate threads every ten seconds
3) responses are saves in responses/results and failures are logged to reponses/fails.txt

averageClients.py
1) Finds the average runtime and success rates for startClients.py

startResultsQuery.py (uses queryResultsTable)
1) Starts a process to query the results table every ten seconds
2) Records the execution time as lines in responses/responses.txt
3) Should be run on the Broker Server

averageResultsQuery.py
1) Collects data from startResultsQuery and finds the average query time

queryResultsTable.sh
1) Sends a query for the last execution of results, and outputs the time taken to the end of responses/responses.txt
2) Should be run on the broker server

averageResultTime
1) Uses the responses from startResultsQuery to find the average query time for the results table

